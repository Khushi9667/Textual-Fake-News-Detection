{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKs3n9A1z63D"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czS2xW3wzGpo"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install numpy==1.26.4\n",
        "!pip install nltk==3.9.1\n",
        "!pip install scipy==1.11.4\n",
        "!pip install gensim==4.3.2\n",
        "!pip install contractions==0.1.73\n",
        "!pip install pandas==2.2.2\n",
        "!pip install scikit-learn==1.6.1\n",
        "!pip install tensorflow==2.16.1\n",
        "!pip install lime shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqEkqqCf6WXK"
      },
      "source": [
        "# Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Enytum8vTY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25G2GZin83ZW"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixl_NWbZ83jx"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjjrYbV76hPy"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMsN152h86gT"
      },
      "outputs": [],
      "source": [
        "#Dataset 1 : LIAR\n",
        "train_data = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
        "val_data = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
        "test_data = pd.read_csv('test.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXq95eGvE_kR"
      },
      "outputs": [],
      "source": [
        "print(\"Training set:\", train_data.shape)\n",
        "print(\"Validation set:\", val_data.shape)\n",
        "print(\"Test set:\", test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9yIdUH_9Dn7"
      },
      "outputs": [],
      "source": [
        "#Dataset 2 : ISOT\n",
        "true_data = pd.read_csv('/content/True.csv')\n",
        "fake_data = pd.read_csv('/content/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb-P-1IU9Fe8"
      },
      "outputs": [],
      "source": [
        "true_data['label'] = 1\n",
        "fake_data['label'] = 0\n",
        "combined_data = pd.concat([true_data, fake_data], axis=0, ignore_index=True)\n",
        "combined_data = combined_data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzNKNzoC6yck"
      },
      "source": [
        "# Train-Validation-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxHxgPsj9OlO"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data2, temp_data = train_test_split(\n",
        "    combined_data,\n",
        "    train_size=0.7,\n",
        "    random_state=42,\n",
        "    stratify=combined_data['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gAnU-9Z-dUa"
      },
      "outputs": [],
      "source": [
        "val_data2, test_data2 = train_test_split(\n",
        "    temp_data,\n",
        "    train_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=temp_data['label']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIh_mvaT-jwo"
      },
      "outputs": [],
      "source": [
        "print(\"Training set:\", train_data2.shape)\n",
        "print(\"Validation set:\", val_data2.shape)\n",
        "print(\"Test set:\", test_data2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYXWjmzZ-4AC"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjrVYtp0-6x9"
      },
      "outputs": [],
      "source": [
        "train_data2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNdZv0Jp65QT"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBTsI4YT-qKr"
      },
      "outputs": [],
      "source": [
        "columns = ['id', 'label', 'statement', 'subject(s)', 'speaker',\n",
        "           'speaker_job_title', 'state info', 'party',\n",
        "           'barely_true_count', 'false_count',\n",
        "           'half_true_count', 'mostly_true_count',\n",
        "           'pants_on_fire_count', 'context']\n",
        "for df in [train_data, val_data, test_data]:\n",
        "    df.columns = columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGQtzQoA_Ihp"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IGD63U47_QW"
      },
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LIAR Dataset Shape:\")\n",
        "print(\"Training set:\", train_data.shape)\n",
        "print(\"Testing set:\", test_data.shape)\n",
        "print(\"Validation set:\", val_data.shape)\n",
        "print(\"Columns:\", train_data.columns.tolist())\n",
        "print(train_data.head())"
      ],
      "metadata": {
        "id": "oB93bcb1JkXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ISOT Dataset Shape:\")\n",
        "print(\"Training set:\", train_data2.shape)\n",
        "print(\"Testing set:\", test_data2.shape)\n",
        "print(\"Validation set:\", val_data2.shape)\n",
        "print(\"Columns:\", train_data2.columns.tolist())\n",
        "print(train_data2.head())"
      ],
      "metadata": {
        "id": "ot11CLASJm6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values in LIAR:\")\n",
        "print(train_data.isnull().sum() + test_data.isnull().sum() + val_data.isnull().sum())"
      ],
      "metadata": {
        "id": "spZQuXgFJpCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Missing Values in ISOT:\")\n",
        "print(train_data2.isnull().sum() + test_data2.isnull().sum() + val_data2.isnull().sum())"
      ],
      "metadata": {
        "id": "S01rKTh0JsNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplicate Rows in LIAR:\")\n",
        "print(train_data.duplicated().sum() + test_data.duplicated().sum() + val_data.duplicated().sum())"
      ],
      "metadata": {
        "id": "isX8djoiJusG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Duplicate Rows in ISOT:\")\n",
        "print(train_data2.duplicated().sum() + test_data2.duplicated().sum() + val_data2.duplicated().sum())"
      ],
      "metadata": {
        "id": "1wWj4AeJJx4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "liar = pd.concat([train_data, test_data, val_data], axis=0, ignore_index=True)\n",
        "isot = pd.concat([train_data2, test_data2, val_data2], axis=0, ignore_index=True)\n",
        "sns.countplot(x='label', data=liar, ax=axes[0])\n",
        "axes[0].set_title(\"LIAR Label Distribution\")\n",
        "sns.countplot(x='label', data=isot, ax=axes[1])\n",
        "axes[1].set_title(\"ISOT Label Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L4lQEnxbJ7N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LIAR Label Counts:\")\n",
        "print(train_data['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "3dLZB7BnJ9Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ISOT Label Counts:\")\n",
        "print(train_data2['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "d-dcmqfdKCsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_train = pd.concat([train_data, train_data2], axis=0, ignore_index=True)\n",
        "combined_test = pd.concat([test_data, test_data2], axis=0, ignore_index=True)\n",
        "combined_val = pd.concat([val_data, val_data2], axis=0, ignore_index=True)\n",
        "print(\"Combined Train Dataset Shape:\", combined_train.shape)\n",
        "print(\"Combined Test Dataset Shape:\", combined_test.shape)\n",
        "print(\"Combined Validation Dataset Shape:\", combined_val.shape)"
      ],
      "metadata": {
        "id": "wO-AsC5TKE48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='label', data=combined_train)\n",
        "plt.title(\"Combined Dataset Label Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HL_QXOzoKHSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Combined Label Distribution:\")\n",
        "print(combined_train['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "pT0SRr0TKOzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACD0zADSKhiv"
      },
      "outputs": [],
      "source": [
        "combined_train['text_length'] = combined_train['statement'].apply(lambda x: len(str(x).split()))\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(data=combined_train, x='label', y='text_length')\n",
        "plt.title(\"Text Length by Label (Combined)\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Number of Words\")\n",
        "plt.ylim(0, 500)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSCZJtr56_mg"
      },
      "source": [
        "# Data Cleaning and Missing Value Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8bVeNYS_KL7"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, val_data, test_data]:\n",
        "    df.drop(['id', 'subject(s)', 'state info', 'party', 'speaker', 'speaker_job_title', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count',\n",
        "             'pants_on_fire_count', 'context'], axis=1, inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.replace('', np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVi_xkh_O5p"
      },
      "outputs": [],
      "source": [
        "print(train_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0HXGAkcIlDW"
      },
      "outputs": [],
      "source": [
        "for df in [train_data2, val_data2, test_data2]:\n",
        "    df['statement'] = df.apply(lambda row: f\"{row['title']}. {row['text']}\", axis=1)\n",
        "    df.drop(['title', 'text', 'date', 'subject'], axis=1, inplace=True)\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df.replace('', np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiF6lcq8I4W4"
      },
      "outputs": [],
      "source": [
        "print(train_data2.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOkSWuNI7FYX"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EItbKScvJDmZ"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, val_data, test_data, train_data2, val_data2, test_data2]:\n",
        "    df['statement'] = df['statement'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ruND-86JIZ8"
      },
      "outputs": [],
      "source": [
        "import contractions\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mkAc1M4JLR3"
      },
      "outputs": [],
      "source": [
        "def sentence_tokenize(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return \" \".join(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcpBaqT_JO6l"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    return ' '.join([word for word in words if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gev47YbBJU_g"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGaX1gg8JYAS"
      },
      "outputs": [],
      "source": [
        "def remove_repeating_chars(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1\\1', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nMWLE3gJaYe"
      },
      "outputs": [],
      "source": [
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www\\S+', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pxw7ZKvoJcre"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43PpnzqNJfJK"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = sentence_tokenize(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_repeating_chars(text)\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFdivzneJh4b"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, val_data, test_data, train_data2, test_data2, val_data2]:\n",
        "    df['statement'] = df['statement'].astype(str).apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDVHeJ6u7NXV"
      },
      "source": [
        "# Tokenization, Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPkkjcRvG2xU"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "lm = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14VYfmbGKHzG"
      },
      "outputs": [],
      "source": [
        "def stem_words(tokens):\n",
        "    return [ps.stem(word) for word in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIuYPB6kKKK5"
      },
      "outputs": [],
      "source": [
        "def lemmatize_words(tokens):\n",
        "    return [lm.lemmatize(word) for word in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvMwcc6IKNFr"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, test_data, val_data, train_data2, test_data2, val_data2]:\n",
        "    df['statement'] = df['statement'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "har7UuxS7dYm"
      },
      "source": [
        "# Word Embedding (Word2Vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eI-MZCRKc8R"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "for df in [train_data, test_data, val_data, train_data2, test_data2, val_data2]:\n",
        "    df['tokens'] = df['statement'].apply(lambda x: word_tokenize(str(x).lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1mGSZOsLn5y"
      },
      "outputs": [],
      "source": [
        "w2v_model = Word2Vec(sentences=train_data['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "w2v_model = Word2Vec(sentences=train_data2['tokens'], vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nPPFjpTKkyo"
      },
      "outputs": [],
      "source": [
        "def sentence_vector(sentence, model):\n",
        "    words = [word for word in sentence if word in model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean([model.wv[word] for word in words], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCb8HDw9KiT9"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, test_data, val_data, train_data2, test_data2, val_data2]:\n",
        "    df['vector'] = df['tokens'].apply(lambda x: sentence_vector(x, w2v_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxO61OmpKgz8"
      },
      "outputs": [],
      "source": [
        "print(train_data[['statement', 'vector']].head())\n",
        "print(train_data2[['statement', 'vector']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieVpBEpM7hZp"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GjhvE_uHTB4"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, test_data, val_data]:\n",
        "    df['label'] = pd.Categorical(df['label']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xvpaKObGoqt"
      },
      "outputs": [],
      "source": [
        "def convert_to_binary(y):\n",
        "     return np.where(y.isin([3, 5]), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzn45-G3Gw7g"
      },
      "outputs": [],
      "source": [
        "for df in [train_data, test_data, val_data]:\n",
        "    df['label'] = convert_to_binary(df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqmGQTYYF4-I"
      },
      "outputs": [],
      "source": [
        "merged_train_data = pd.concat([train_data, train_data2], axis=0, ignore_index=True)\n",
        "merged_train_data = merged_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Merged train data shape:\", merged_train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQnbiaklI-l3"
      },
      "outputs": [],
      "source": [
        "print(merged_train_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjadZZ-VGJ3R"
      },
      "outputs": [],
      "source": [
        "merged_test_data = pd.concat([test_data, test_data2], axis=0, ignore_index=True)\n",
        "merged_test_data = merged_test_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Merged test data shape:\", merged_test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBDM-BrdGCYY"
      },
      "outputs": [],
      "source": [
        "merged_val_data = pd.concat([val_data, val_data2], axis=0, ignore_index=True)\n",
        "merged_val_data = merged_val_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "print(\"Merged validation data shape:\", merged_val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFzm_fOhFhut"
      },
      "outputs": [],
      "source": [
        "X_train_text = np.array(merged_train_data['vector'].tolist())\n",
        "X_val_text = np.array(merged_val_data['vector'].tolist())\n",
        "X_test_text = np.array(merged_test_data['vector'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyGqAnQuGaIV"
      },
      "outputs": [],
      "source": [
        "y_train_binary = merged_train_data['label'].values\n",
        "y_val_binary = merged_val_data['label'].values\n",
        "y_test_binary = merged_test_data['label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2OOx0TM7igx"
      },
      "source": [
        "# Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxCfp8qOHHUR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D, Reshape, GRU, LSTM, Bidirectional,\n",
        "                                     MultiHeadAttention, LayerNormalization, Attention)\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD2btYw0ESc6"
      },
      "outputs": [],
      "source": [
        "text_input = Input(shape=(100,), name='text_input')\n",
        "text_reshaped = Reshape((100, 1))(text_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBodFmqEESf0"
      },
      "outputs": [],
      "source": [
        "conv_layer = Conv1D(filters=512, kernel_size=5, activation='relu')(text_reshaped)\n",
        "pooling_layer = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "conv_layer2 = Conv1D(filters=512, kernel_size=3, activation='relu')(pooling_layer)\n",
        "pooling_layer2 = MaxPooling1D(pool_size=2)(conv_layer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFZ2FLU2ESjL"
      },
      "outputs": [],
      "source": [
        "attention_output = MultiHeadAttention(num_heads=4, key_dim=64)(pooling_layer2, pooling_layer2)\n",
        "attention_output = LayerNormalization()(attention_output + pooling_layer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D21irJojESmX"
      },
      "outputs": [],
      "source": [
        "bi_lstm_layer = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(attention_output)\n",
        "bi_lstm_layer = BatchNormalization()(bi_lstm_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULoLqlkyEGV5"
      },
      "outputs": [],
      "source": [
        "bi_gru_layer = Bidirectional(GRU(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2))(bi_lstm_layer)\n",
        "bi_gru_layer = BatchNormalization()(bi_gru_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucmbf4C8EGd5"
      },
      "outputs": [],
      "source": [
        "attention_output1 = Attention()([bi_gru_layer, bi_gru_layer])\n",
        "attention_output1 = GlobalAveragePooling1D()(attention_output1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcM1IzbsEGn7"
      },
      "outputs": [],
      "source": [
        "combined = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(attention_output1)\n",
        "combined = Dropout(0.6)(combined)\n",
        "combined = BatchNormalization()(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c8zmTWyEGrl"
      },
      "outputs": [],
      "source": [
        "output_layer = Dense(1, activation='sigmoid')(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMryiMeCEGzr"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=text_input, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF_L_Q0r7saG"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYjKMRdOHQXt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrV_s9DCDuJn"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train_text, y_train_binary,\n",
        "    validation_data=(X_val_text, y_val_binary),\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdZZhFz571px"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtkuXK8DDfnG"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_text, y_test_binary)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y5gQK6MDjZB"
      },
      "outputs": [],
      "source": [
        "y_pred_probs = model.predict(X_test_text)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DrjaBN2Dd-N"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_binary, y_pred, target_names=[\"True\", \"Fake\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8NEbvgh76CQ"
      },
      "source": [
        "# Performance Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMmza5PMDSod"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, ConfusionMatrixDisplay, roc_curve, auc, average_precision_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojBmWsq8DFdJ"
      },
      "outputs": [],
      "source": [
        "precision, recall, _ = precision_recall_curve(y_test_binary, y_pred_probs)\n",
        "avg_precision = average_precision_score(y_test_binary, y_pred_probs)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, lw=2, color='purple', label=f'AP = {avg_precision:.2f}')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbQhpPRdDH_v"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_test_binary, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anC7QO63C9La"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_test_binary, y_pred_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e8Sr-zTSSMH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_test_binary, y_pred),\n",
        "    \"Precision\": precision_score(y_test_binary, y_pred),\n",
        "    \"Recall\": recall_score(y_test_binary, y_pred),\n",
        "    \"F1-score\": f1_score(y_test_binary, y_pred)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(metrics.keys(), metrics.values(), color='skyblue')\n",
        "plt.title(\"Performance Metrics\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDGfAgQ5KvRy"
      },
      "source": [
        "# Explainable AI (LIME + SHAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPsbd9RIKyg6"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "def _tokenize_for_model(text: str):\n",
        "    cleaned = preprocess_text(str(text))\n",
        "    return word_tokenize(cleaned)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def texts_to_vectors(texts):\n",
        "    vecs = []\n",
        "    for t in texts:\n",
        "        toks = _tokenize_for_model(t)\n",
        "        vecs.append(sentence_vector(toks, w2v_model))\n",
        "    return np.vstack(vecs)"
      ],
      "metadata": {
        "id": "jk4PXhVwYfvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba_texts(texts):\n",
        "    X = texts_to_vectors(texts)\n",
        "    probs = model.predict(X, verbose=0).reshape(-1)\n",
        "    return np.vstack([1.0 - probs, probs]).T"
      ],
      "metadata": {
        "id": "psY31DRBYaa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_pos(texts):\n",
        "    X = texts_to_vectors(texts)\n",
        "    return model.predict(X, verbose=0).reshape(-1)"
      ],
      "metadata": {
        "id": "WcEX0B0fYURo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = merged_test_data['statement'].iloc[0]\n",
        "print(\"Example text:\", example_text[:300], \"...\" if len(example_text)>300 else \"\")"
      ],
      "metadata": {
        "id": "YSR-bGx-YKxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_NAMES = [\"Fake\", \"True\"]\n",
        "lime_explainer = LimeTextExplainer(class_names=CLASS_NAMES)\n",
        "lime_exp = lime_explainer.explain_instance(\n",
        "    example_text,\n",
        "    predict_proba_texts,\n",
        "    num_features=12,\n",
        "    top_labels=1\n",
        ")"
      ],
      "metadata": {
        "id": "d1y1boMPYHBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLIME top features per label:\")\n",
        "for label in lime_exp.available_labels():\n",
        "    print(f\"\\nLabel {label}:\")\n",
        "    for w, wgt in lime_exp.as_list(label=label):\n",
        "        print(f\"{w:>20s}  {wgt:+.4f}\")"
      ],
      "metadata": {
        "id": "8tT7iNsdW1XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lime_explanation.html\",\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(lime_exp.as_html())\n",
        "print(\"\\nSaved: lime_explanation.html (open/download to view interactive LIME)\")"
      ],
      "metadata": {
        "id": "oac7cJofWu5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "shap.initjs()\n",
        "text_masker = shap.maskers.Text()\n",
        "shap_explainer = shap.Explainer(predict_pos, text_masker)\n",
        "shap_values = shap_explainer([example_text])"
      ],
      "metadata": {
        "id": "sngMNvbsWWxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    shap.plots.text(shap_values[0])\n",
        "except Exception:\n",
        "    shap.plots.bar(shap_values[0], max_display=12)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PB4-12MsWQYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "background_summary = shap.kmeans(X_train_text, 50)\n",
        "te_size = min(20, len(X_test_text))\n",
        "test_sample = X_test_text[:te_size]\n",
        "kernel_explainer = shap.KernelExplainer(lambda X: model.predict(X).reshape(-1), background_summary)\n",
        "shap_vals_num = kernel_explainer.shap_values(test_sample, nsamples=100)"
      ],
      "metadata": {
        "id": "jD4NsNITWBkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if isinstance(shap_vals_num, list):\n",
        "    shap_vals_num = shap_vals_num[0]"
      ],
      "metadata": {
        "id": "wCk9zspAV9wU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.summary_plot(\n",
        "    shap_vals_num,\n",
        "    test_sample,\n",
        "    feature_names=[f\"dim_{i}\" for i in range(X_train_text.shape[1])]\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2Xzye5zOV0gV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
